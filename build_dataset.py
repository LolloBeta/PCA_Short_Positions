# -*- coding: utf-8 -*-
"""build_dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XPPhRuO4Po0FOtbV0wUy-_2xq9Jug8za
"""

import argparse
import time
from pathlib import Path
import pandas as pd
import numpy as np
from datetime import datetime
from typing import List, Tuple
import warnings

import yfinance as yf
from tqdm import tqdm

def parse_args(): # Parse command line arguments
    parser = argparse.ArgumentParser(description='Build S&P 500 dataset from Yahoo Finance')
    parser.add_argument('--intervals', required=True, help='CSV with columns: ticker,start_date,end_date')
    parser.add_argument('--outdir', default='./data', help='Output directory')
    parser.add_argument('--start', type=str, help='Force global start date (YYYY-MM-DD)')
    parser.add_argument('--end', type=str, help='Force global end date (YYYY-MM-DD)')
    parser.add_argument('--retry', type=int, default=2, help='Number of retries for downloads')
    parser.add_argument('--sleep', type=float, default=0.5, help='Sleep between requests (seconds)')
    return parser.parse_args()

def normalize_ticker(ticker: str) -> str: # Normalize ticker for Yahoo Finance (replace . with -)
    return ticker.strip().upper().replace('.', '-') if ticker else ticker

def load_intervals(path: Path) -> pd.DataFrame: # Load membership intervals from CSV
    df = pd.read_csv(path, parse_dates=['start_date', 'end_date'])
    df['ticker'] = df['ticker'].astype(str).apply(normalize_ticker)
    return df.sort_values(['ticker', 'start_date']).reset_index(drop=True)

def merge_intervals(df_ticker: pd.DataFrame) -> pd.DataFrame: # Merge overlapping or contiguous intervals for a single ticker
    if df_ticker.empty:
        return df_ticker

    df_ticker = df_ticker.sort_values('start_date').reset_index(drop=True)
    merged = []
    current_start = df_ticker.iloc[0]['start_date']
    current_end = df_ticker.iloc[0]['end_date']

    for _, row in df_ticker.iloc[1:].iterrows():
        if row['start_date'] <= current_end + pd.Timedelta(days=1):
            current_end = max(current_end, row['end_date'])
        else:
            merged.append((current_start, current_end))
            current_start, current_end = row['start_date'], row['end_date']

    merged.append((current_start, current_end))
    result = pd.DataFrame(merged, columns=['start_date', 'end_date'])
    result['ticker'] = df_ticker.iloc[0]['ticker']
    return result[['ticker', 'start_date', 'end_date']]

def download_ticker(ticker: str, start: pd.Timestamp, end: pd.Timestamp, retry: int, sleep: float) -> pd.DataFrame: # Download daily data for a ticker
    for attempt in range(retry + 1):
        try:
            df = yf.download(ticker, start=start.date(), end=(end + pd.Timedelta(days=1)).date(),
                           interval='1d', progress=False, auto_adjust=False, actions=False)
            if isinstance(df, pd.DataFrame) and not df.empty:
                df.index = pd.to_datetime(df.index).tz_localize(None)
                return df.rename(columns=str.title)
        except Exception:
            pass
        time.sleep(sleep * (attempt + 1))
    warnings.warn(f"Failed to download {ticker}")
    return pd.DataFrame()

def filter_by_intervals(df: pd.DataFrame, intervals: List[Tuple[pd.Timestamp, pd.Timestamp]]) -> pd.DataFrame: # Keep only dates within specified intervals.
    if df.empty:
        return df
    mask = np.zeros(len(df), dtype=bool)
    for start, end in intervals:
        mask |= (df.index >= start) & (df.index <= end)
    return df[mask].copy()

def main():
    args = parse_args()
    outdir = Path(args.outdir) # Setup output directory
    outdir.mkdir(parents=True, exist_ok=True)
    (outdir / 'logs').mkdir(exist_ok=True)
    df_intervals = load_intervals(Path(args.intervals)) # Load and process intervals

    if args.start:
        start_date = pd.to_datetime(args.start)
        df_intervals['start_date'] = df_intervals['start_date'].clip(lower=start_date)
    if args.end:
        end_date = pd.to_datetime(args.end)
        df_intervals['end_date'] = df_intervals['end_date'].clip(upper=end_date)

    merged_intervals = pd.concat([merge_intervals(grp) for _, grp in df_intervals.groupby('ticker')], ignore_index=True) # Merge overlapping intervals per ticker
    records = []
    failed = []

    for ticker, grp in tqdm(merged_intervals.groupby('ticker'), desc='Tickers'): # Download data
        intervals = [(row['start_date'], row['end_date']) for _, row in grp.iterrows()]
        start = min(s for s, _ in intervals)
        end = max(e for _, e in intervals)
        df_price = download_ticker(ticker, start, end, args.retry, args.sleep)
        if df_price.empty:
            failed.append(ticker)
            continue
        df_price = filter_by_intervals(df_price, intervals)
        if df_price.empty:
            continue
        df_price = df_price.reset_index()
        df_price.columns = ['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']
        df_price['ticker'] = ticker
        records.append(df_price)

    if records: # Create long format dataframe
        prices_long = pd.concat(records, ignore_index=True).sort_values(['ticker', 'Date'])
    else:
        prices_long = pd.DataFrame(columns=['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'ticker'])
    long_path = outdir / 'sp500_prices_long.csv' # Save long format
    prices_long.to_csv(long_path, index=False)

    if not prices_long.empty:
        adj_close_wide = prices_long.pivot_table(index='Date', columns='ticker', values='Adj Close', aggfunc='last') # Create wide format
        wide_path = outdir / 'sp500_adj_close_wide.csv' # Save wide format
        adj_close_wide.to_csv(wide_path)
        print(f"Saved wide format: {wide_path}")

    if failed: # Create wide format
        (outdir / 'logs' / 'failed_tickers.txt').write_text('\n'.join(sorted(set(failed))))
        print(f"Failed tickers: {len(set(failed))} (see logs/failed_tickers.txt)")

    print("\nDataset build complete")